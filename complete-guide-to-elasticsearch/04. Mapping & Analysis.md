### 37. analysis 소개

#### Analysis

* text analysis라고 불린다.
* text  필드/값에 적용된다.
* text 값은 문서를 인덱싱할 때 분석된다.
* 결과는 효과적인 검색을 위해  데이터 구조에 저장된다.
* _source 객체는 문서 검색할 때는 사용되지 않는다.
    * 문서 인덱싱할 때 정확한 값을 포함한다.



<img src="images/image-20220815202343107.png" alt="image-20220815202343107" style="zoom:50%;" />

* analyzer는 3가지 블록으로 구성되어 있다. (character filters, tokenizer, token filters)
* text를 analyzing하는 결과는 검색가능한 데이터 구조에 저장된다.



#### Character filters

* 문자를 추가, 삭제, 변경한다.
* Analyzer는 0개 이상의 character filter를 포함한다.
* character filter는 정의된 순서대로 적용된다.
* 예(html_stripe 필터)
    * Input: "I&apos;m in a \<em>good\</em> mood\&nbsp;-\&nbsp;and I \<strong> love \</strong> açaí!"
    * Output: "I'm in a good mood - and I love açaí!"



#### Tokenizers

* Analyzer는 하나의 tokenizer를 포함한다.
* 문자를 토크나이징한다. 예) 토큰으로 짜른다.
* 문자는 토큰의 부분으로 자른다.
* 예
    * Input: "I REALLY like beer!"
    * Output: ["I", "REALLY", "like", "beer"]



#### Token filters

* 입력으로 토크나이저이ㅡ 결과를 받는다. (Ex: 토큰)
* token filter는 토큰을 추가, 수정, 삭제할 수 있다.
* analyzer는 0개 이상의 token filter를 가진다.
* token filter는 정의된 순서대로 적용된다.
* 예: lowercase filter
    * Input: ["I", "REALLY", "like", "beer"]
    * Output: ["i", "really", "like", "beer"]



#### built-in, custom 컨포넌트

* Built-in analyzer, character filters, token filters를 사용할 수 있다.
* 또한 Custom analyzer를 사용할 수 있다.



<img src="images/image-20220815204429675.png" alt="image-20220815204429675" style="zoom:50%;" />



### 38. Analyzer API 사용하기

아래 문장을 analyzer로 실행하면 standard analyzer로 실행이 된다.

```
POST _analyze
{
  "text": "2 guys walk into   a bar, but the third... DUCKS! :-)",
  "analyzer": "standard"
}
```

Standard analyzer는 특수문자는 모두 제거한다.

```
{
  "tokens" : [
    {
      "token" : "2",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<NUM>",
      "position" : 0
    },
    {
      "token" : "guys",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "walk",
      "start_offset" : 7,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "into",
      "start_offset" : 12,
      "end_offset" : 16,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "a",
      "start_offset" : 19,
      "end_offset" : 20,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "bar",
      "start_offset" : 21,
      "end_offset" : 24,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "but",
      "start_offset" : 26,
      "end_offset" : 29,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "the",
      "start_offset" : 30,
      "end_offset" : 33,
      "type" : "<ALPHANUM>",
      "position" : 7
    },
    {
      "token" : "third",
      "start_offset" : 34,
      "end_offset" : 39,
      "type" : "<ALPHANUM>",
      "position" : 8
    },
    {
      "token" : "ducks",
      "start_offset" : 43,
      "end_offset" : 48,
      "type" : "<ALPHANUM>",
      "position" : 9
    }
  ]
}

```

이는 아래와 같다.

```
POST _analyze
{
  "text": "2 guys walk into   a bar, but the third... DUCKS! :-)",
  "char_filter": [], 
  "tokenizer": "standard", 
  "filter": ["lowercase"]
}
```



### 39. 역색인(inverted indices) 이해하기

#### 소개

* 필드값들이 여라가지 데이터 구조 중 하나로 저장된다.
    * 데이터 구조는 필드 데이터 유형에 따라 다르다.
* 효율적인 데이타 접근을 보장한다 - 예. 검색
* Elasticsearch가 아니라 아파치 루씬이 처리한다.
* 이번 내용은 역색인에 초점을 두고 있다.



#### 역색인

* 텀(term) 사이 매핑과 어느 문서가 포함하는가
* 분석기 외부 관점에서는 텀(term)이라는 용어를 사용한다.
    * 토큰(token)이라는 용어는 분석기 관점에서만 사용된다.
    * 대부분은 텀(term)이라고 사용한다.
* 텀(term)은 알파벳순으로 정렬되어 있다.
* 역색인은 단순히 텀(term)과 문서 ID보다 더 많은 정보를 가지고 있다.
    * 예) 스코어를 위한 정보
* 하나의 텍스트 필드 당 하나의 역색인 파일이 생긴다.
* 예를 들면 다른 데이터 타입은 BKD 트리를 사용한다. (예: 숫자, 날짜, 위치)



<img src="images/image-20220816214850962.png" alt="image-20220816214850962" style="zoom:50%;" />

* ducks 를 검색할 때 역색인에서 어느 문서가 ducks를 포함하고 있는지 확인하면 된다. (#1, #3)



<img src="images/image-20220816220056548.png" alt="image-20220816220056548" style="zoom:50%;" />

* 위와 같이 다른 2개의 문서를 인덱싱을 하면 역색인 파일은 각 필드별로 각각 만들어진다. (name 필드, description 필드)



### 40. Mapping 소개

#### mapping은 무엇인가?

* 문서 구조를 정의한다 (예: 필드와 데이터 타입)
    * 또한 값이 어떻게 인덱싱되는지 구성하는데 사용
* 관계형 DB에서 테이블 스키마와 유사
* 명시적인 매핑
    * 우리가 직접 필드 매핑을 정의
* 동적 매핑
    * elasticsearch가 필드 매핑을 정의한다.



### 41. 데이터 타입

object, text, float, boolean, interger, double, short, date



#### object 데이터 타입

* JSON object에 사용
* objects는 nested가 될 수 있다.
* Properties 파라미터를 사용하여 매핑된다.
* objects는 루씬에 객체로 저장되지 않는다.
    * objects는 유효한 JSON을 인덱싱할 수 있도록 변형된다.
    * 특별한 경우에 objects는 flatten된다.



<img src="images/image-20220817193556320.png" alt="image-20220817193556320" style="zoom:50%;" />

객체 array인 경우는 아래처럼 저장된다.

![image-20220817194127313](images/image-20220817194127313.png)

```
QUERY: MATCH products WHERE review.author == "John Doe" AND reviews.rating >= 4.0
```

위와 같이 쿼리를 실행하면 해당 문서가 검색되는 것으로 나온다. 하지만 실제로는 John Doe는 3.5이라서 검색되지 않아야 한다.

이는 우리가 원하는 결과가 아니다.



#### nested 데이터 타입

* Object 데이터 타입과 유사하지만 객체간의 관계를 유지한다.
    * objects 배열을 인덱싱할 때 유용하다.
* 독립적으로 objects를 쿼리할 수 있게 한다.
    * nested 쿼리를 사용해야 한다.
* nested object는 숨겨진 문서로 저장된다.

<img src="images/image-20220817195153516.png" alt="image-20220817195153516" style="zoom:50%;" />

```
QUERY: MATCH products WHERE review.author == "John Doe" AND reviews.rating >= 4.0
```

객체는 독립적으로 저장되기 때문에 쿼리는 예상대로 동작한다.



루씬은 object의 개념이 없기 때문에 이러한 객체는 어떻게 저장될까?

<img src="images/image-20220817195732582.png" alt="image-20220817195732582" style="zoom:50%;" />

위와 같이 review를 10개 가진 하나의 문서(product)를 저장하면 총 11개의 문서가 저장된다. (1개의 product + 10개의 review)



#### keyword 데이터 타입

* 정확한 값이 일치할 때 사용
* 일반적으로 필터링, 집합(aggregation), 정렬에 사용된다.
* 예) PUBLISHED 상태의 기사를 검색
* Full-text 검색을 위해서는 text 데이터 타입을 사용해라.
    * 예) 기사 내용을 검색



### 42. Keyword 데이터 유형은 어떻게 동작하는가?

#### keyword 필드는 어떻게 분석되는가?

* Keyword 필드는 keyword analyzer로 분석된다.
* keyword analyzer는 no-op analyzer이다. (어떤 것도 하지 않는다는 뜻)
    * 단일 토큰으로 수정되지 않은 문자열을 출력
    * 이 토큰이 역색인 파일에 추가된다.
* keyword 필드는 정확히 일치, 집합(aggregation), 정렬에 사용된다.

```
POST _analyze
{
  "text": "2 guys walk into   a bar, but the third... DUCKS! :-)",
  "analyzer": "keyword"
}
```

```json
{
  "tokens" : [
    {
      "token" : "2 guys walk into   a bar, but the third... DUCKS! :-)",
      "start_offset" : 0,
      "end_offset" : 53,
      "type" : "word",
      "position" : 0
    }
  ]
}
```



### 43. type coercion 이해

* 데이터 유형이 문서 인덱싱될 때 확인된다.
    * 검증되고 어떤 값들은 실패한다.
    * 예) text 필드에 객체 인덱싱할 때
* 때로는, 잘못된 데이터 타입도 가능하다.



아래 쿼리를 차례대로 실행하자.

```
PUT coercion_test/_doc/1
{
  "price": 7.4
}
```

price에 float 타입으로 자동으로 지정된다.

```
PUT coercion_test/_doc/2
{
  "price": "7.4"
}
```

"7.4" 문자열 입력 시 7.4의 float타입으로 변경되어 저장된다.

```
PUT coercion_test/_doc/3
{
  "price": "7.4m"
}
```

"7.4m" 문자열 입력 시 float 타입으로 변경을 할 수 없으므로 실패한다.



조회를 해보자.

```
GET coercion_test/_doc/1
```

```
{
  "_source" : {
     "price" : 7.4
  }
}
```

7.4 float타입으로 조회된다.

2번 문서를 조회하면

```
GET coercion_test/_doc/2
```

```
{
  "_source" : {
    "price" : "7.4"
  }
}
```

"7.4" 문자열로 저장된다.



#### _source object의 의해

* 인덱싱할 때 제공된 값을 가진다. ("7.4")
    * 인덱싱된 값이 아니다 (7.4)
* 검색 쿼리는 인덱싱된 값을 사용한다. (_source값이 아니다)
    * BKD tree, 역색인, etc
* _source는 갑싱 어떻게 인덱싱되었는지를 나타내지 않는다.
    * _source에서 값을 사용하려면 coercion을 유지해라.
    * 이 예제에서는 string 혹은 float이 될 수 있다.



#### 몇 가지 더

* Integer 필드에 floating point를 넣으면 integer로 들어갈 것이다.
* coercion은 동적 매핑에서는 사용되지 않는다.
    * 새 필드에 "7.4"를 넣으면 text 매핑을 만들것이다.
* 항상 올바른 데이터 유형을 사용하도록 노력해라.
    * 특히 필드를 처음 인덱싱 할 때
* coercion을 비활성화 하는 것은 선택사항이다.
    * 기본값은 enabled이다.



### 44. Arrays 이해















