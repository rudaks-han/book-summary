### 37. analysis 소개

#### Analysis

* text analysis라고 불린다.
* text  필드/값에 적용된다.
* text 값은 문서를 인덱싱할 때 분석된다.
* 결과는 효과적인 검색을 위해  데이터 구조에 저장된다.
* _source 객체는 문서 검색할 때는 사용되지 않는다.
    * 문서 인덱싱할 때 정확한 값을 포함한다.



<img src="images/image-20220815202343107.png" alt="image-20220815202343107" style="zoom:50%;" />

* analyzer는 3가지 블록으로 구성되어 있다. (character filters, tokenizer, token filters)
* text를 analyzing하는 결과는 검색가능한 데이터 구조에 저장된다.



#### Character filters

* 문자를 추가, 삭제, 변경한다.
* Analyzer는 0개 이상의 character filter를 포함한다.
* character filter는 정의된 순서대로 적용된다.
* 예(html_stripe 필터)
    * Input: "I&apos;m in a \<em>good\</em> mood\&nbsp;-\&nbsp;and I \<strong> love \</strong> açaí!"
    * Output: "I'm in a good mood - and I love açaí!"



#### Tokenizers

* Analyzer는 하나의 tokenizer를 포함한다.
* 문자를 토크나이징한다. 예) 토큰으로 짜른다.
* 문자는 토큰의 부분으로 자른다.
* 예
    * Input: "I REALLY like beer!"
    * Output: ["I", "REALLY", "like", "beer"]



#### Token filters

* 입력으로 토크나이저이ㅡ 결과를 받는다. (Ex: 토큰)
* token filter는 토큰을 추가, 수정, 삭제할 수 있다.
* analyzer는 0개 이상의 token filter를 가진다.
* token filter는 정의된 순서대로 적용된다.
* 예: lowercase filter
    * Input: ["I", "REALLY", "like", "beer"]
    * Output: ["i", "really", "like", "beer"]



#### built-in, custom 컨포넌트

* Built-in analyzer, character filters, token filters를 사용할 수 있다.
* 또한 Custom analyzer를 사용할 수 있다.



<img src="images/image-20220815204429675.png" alt="image-20220815204429675" style="zoom:50%;" />



### 38. Analyzer API 사용하기

아래 문장을 analyzer로 실행하면 standard analyzer로 실행이 된다.

```
POST _analyze
{
  "text": "2 guys walk into   a bar, but the third... DUCKS! :-)",
  "analyzer": "standard"
}
```

Standard analyzer는 특수문자는 모두 제거한다.

```
{
  "tokens" : [
    {
      "token" : "2",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<NUM>",
      "position" : 0
    },
    {
      "token" : "guys",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "walk",
      "start_offset" : 7,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "into",
      "start_offset" : 12,
      "end_offset" : 16,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "a",
      "start_offset" : 19,
      "end_offset" : 20,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "bar",
      "start_offset" : 21,
      "end_offset" : 24,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "but",
      "start_offset" : 26,
      "end_offset" : 29,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "the",
      "start_offset" : 30,
      "end_offset" : 33,
      "type" : "<ALPHANUM>",
      "position" : 7
    },
    {
      "token" : "third",
      "start_offset" : 34,
      "end_offset" : 39,
      "type" : "<ALPHANUM>",
      "position" : 8
    },
    {
      "token" : "ducks",
      "start_offset" : 43,
      "end_offset" : 48,
      "type" : "<ALPHANUM>",
      "position" : 9
    }
  ]
}

```

이는 아래와 같다.

```
POST _analyze
{
  "text": "2 guys walk into   a bar, but the third... DUCKS! :-)",
  "char_filter": [], 
  "tokenizer": "standard", 
  "filter": ["lowercase"]
}
```

